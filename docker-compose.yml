networks:
  # This allows OS2datascanners admin and admin_runner containers access 
  # to both the default network in OS2datascanner, and in the OS2mo-project,
  # if available. Else, they will only use os2datascanner_default.
  os2mo_default:
    name:
      os2mo_default

  traefik:
    name: traefik

services:

  init_SBSYS_DB:
    image: magentaaps/os2datascanner-sbsys-init:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: sbsys-init
    depends_on:
      SBSYS_DB:
        condition: service_healthy
    profiles:
      - sbsys

  SBSYS_DB:
    image: mcr.microsoft.com/mssql/server:2022-latest
    ports:
      - "7777:1433"
      - "7778:1434"
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_SA_PASSWORD=ub3rStr0nGpassword
      - MSSQL_COLLATION=SQL_Danish_Pref_CP1_CI_AS
    healthcheck:
      test: [ "CMD-SHELL", "/opt/mssql-tools18/bin/sqlcmd -No -S localhost -U sa -P ${Sa_Password:-ub3rStr0nGpassword} -Q 'SELECT 1' || exit 1" ]
      interval: 10s
      retries: 3
      start_period: 10s
      timeout: 3s
    profiles:
      - sbsys

  db:
    image: postgres:16
    ports:
        - "8035:5432"
    shm_size: '2GB'
    command: >-
      postgres
       -c shared_buffers=1GB
       -c work_mem=64MB
       -c maintenance_work_mem=499MB
       -c effective_cache_size=8GB
    env_file:
      - dev-environment/db.env
    environment:
      - PGUSER=postgres
      - PGDATA=/var/lib/postgresql/16/data
    volumes:
      - postgres-data:/var/lib/postgresql/data # Old data, from PG 12, we can probably remove this after a while / everybody has transitioned.
      - pgdata_16:/var/lib/postgresql/16/data
      - ./docker/postgres-initdb.d/10-test-for-valid-env-variables.sh:/docker-entrypoint-initdb.d/10-test-for-valid-env-variables.sh
      - ./docker/postgres-initdb.d/20-create-admin-db-and-user.sh:/docker-entrypoint-initdb.d/20-create-admin-db-and-user.sh
      - ./docker/postgres-initdb.d/40-create-report-db-and-user.sh:/docker-entrypoint-initdb.d/40-create-report-db-and-user.sh
      - ./dev-environment/postgres-initdb.d/50-add-createdb-permissions.sh:/docker-entrypoint-initdb.d/50-add-createdb-permissions.sh
    healthcheck:
      test: ["CMD-SHELL", "pg_isready || false"]
      interval: 5s
      start_period: 30s
      retries: 3
    depends_on:
      pg_upgrade:
        condition: service_completed_successfully

  pg_upgrade:
    image: magentaaps/postgres-upgrade:master
    environment: # These are passed to the actual running stage. I.e. "up"
      - PGUSER=postgres
      - PGDATA=/var/lib/postgresql/16/data
      - PGNEW=16
      - PGOLD=12
    volumes:
      - postgres-data:/var/lib/postgresql/12/data
      - pgdata_16:/var/lib/postgresql/16/data

  queue:
    # Normally, we expect the `rabbitmq` image. The -management images come
    # with a set of management plugins installed and enabled by default. They
    # can be accessed through a web interface. The credentials are os2ds/os2ds.
    image: rabbitmq:3.12.14-management-alpine
    hostname: os2datascanner_msg_broker
    environment:
      - RABBITMQ_CONFIG_FILE=/etc/rabbitmq/rabbitmq.conf
      - RABBITMQ_ADVANCED_CONFIG_FILE=/etc/rabbitmq/advanced.config
    ports:
      - "8030:15672"  # management port
      - "8072:5672"  # For local attachment during IDE debugging
    volumes:
      - ./docker/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
      - ./docker/rabbitmq/advanced.config:/etc/rabbitmq/advanced.config:ro
      - ./docker/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json:ro
    healthcheck:
      test: ["CMD-SHELL", "rabbitmqctl await_online_nodes 1 || false"]
      interval: 5s
      start_period: 30s
      retries: 3

  frontend:
    build:
      context: .
      # we could also use docker/report/Dockerfile as our dockerfile
      dockerfile: docker/base/Dockerfile
      target: frontend
    volumes:
      # "create" the folder node_modules in the container as otherwise it will
      # be overwritten/removed if ./src/os2datascanner/projects/static on the
      # host does not contain node_modules. node_modules is required for
      # the webpack dev server to run
      - /code/src/os2datascanner/projects/static/node_modules/
      - ./src/os2datascanner/projects/static:/code/src/os2datascanner/projects/static
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/

  admin_migrate:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: ["python", "manage.py", "migrate"]
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      db:
        condition: service_healthy

  admin:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
      args:
       COMMIT_SHA: "${OS2DS_COMMIT_SHA}"
       COMMIT_TAG: "${OS2DS_COMMIT_TAG}"
       CURRENT_BRANCH: "${OS2DS_CURRENT_BRANCH}"
    command: [
      # In production, we use gunicorn with the worker class
      # "uvicorn.workers.UvicornWorker", but that does not work with --reload,
      # so we use uvicorn in development at the cost of some dev/prod parity.
      "uvicorn",
      "--reload",
      "--host", "0.0.0.0",
      "--port", "5000",
      "--reload-dir", "/code/src/os2datascanner",
      # reload when translations are compiled with `docker-compose exec admin django-admin compilemessages`
      "--reload-include", "*.mo",
      "os2datascanner.projects.admin.asgi:application",
    ]
    tty: # If you like coloured log messages.
      true
    volumes:
      # First line below is needed in dev, because we copy all our live code in, which replaces
      # the built-in code from Dockerfile (this is for watchdog reasons)
      # - /code/src/os2datascanner/core_organizational_structure/
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./dev-environment/admin/.secret:/code/.secret
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./dev-environment/uploads:/code/uploads
      - ./dev-environment/admin/ipython:/home/os2ds_admin/.ipython
    ports:
      - "8020:5000"
    extra_hosts:
      # Adds an entry to container's /etc/hosts, allowing it to use localhost-urls.
      # Needed to be able to communicate with keycloak through 'localhost' URL and be able to
      #  test Azure SSO.
      - "localhost:host-gateway"
    depends_on: &admin_dependencies
      admin_migrate:
        condition: service_completed_successfully
      frontend:
        condition: service_started
      queue:
        condition: service_healthy
    networks:
      - default
      - os2mo_default

  admin_checkup_collector:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: python manage.py checkup_collector
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *admin_dependencies

  admin_status_collector:
    restart: unless-stopped
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: python manage.py status_collector
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *admin_dependencies

  admin_job_runner:
    restart: unless-stopped
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    environment:
      - AMQP_BROADCAST_SYNC=false
      - OSDS_DEBUG_OAUTH2=true
    command: python manage.py run_background_jobs
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./dev-environment/uploads:/code/uploads
    depends_on: *admin_dependencies
    stop_grace_period: 1m30s
    extra_hosts:
      - "localhost:host-gateway"
    networks:
      - default
      - os2mo_default

  admin_cron:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: supercronic /code/docker/crontab
    environment:
      - TZ=Europe/Copenhagen
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./dev-environment/uploads:/code/uploads
    depends_on:
      admin_migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      queue:
        condition: service_healthy
    profiles:
      - cron

  report_migrate:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: ["python", "manage.py", "migrate"]
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      db:
        condition: service_healthy

  report:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: [
      # In production, we use gunicorn with the worker class
      # "uvicorn.workers.UvicornWorker", but that does not work with --reload,
      # so we use uvicorn in development at the cost of some dev/prod parity.
      "uvicorn",
      "--reload",
      "--host", "0.0.0.0",
      "--port", "5000",
      "--reload-dir", "/code/src/os2datascanner",
      # reload when translations are compiled with `docker-compose exec report django-admin compilemessages`
      "--reload-include", "*.mo",
      "os2datascanner.projects.report.asgi:application",
    ]
    tty:
      true
    volumes:
      # First line below is needed in dev, because we copy all our live code in, which replaces
      # the built-in code from Dockerfile (this is for watchdog reasons)
      # - /code/src/os2datascanner/core_organizational_structure/
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./src/os2datascanner/projects/report/media:/code/uploads/report
      # used for assertion signage see dev-environment/report/dev-settings.toml
      #- ./ssl/signing.crt:/etc/ssl/certs/signing.crt:ro
      #- ./ssl/signing.key:/etc/ssl/certs/signing.key:ro
      - ./dev-environment/report/ipython:/home/os2ds_report/.ipython
    ports:
      - "8040:5000"
    extra_hosts:
      # Adds an entry to container's /etc/hosts, allowing it to use localhost-urls.
      # Needed to be able to communicate with keycloak through 'localhost' URL and be able to
      #  test Azure SSO.
      - "localhost:host-gateway"
    depends_on: &report_dependencies
      report_migrate:
        condition: service_completed_successfully
      frontend:
        condition: service_started
      queue:
        condition: service_healthy

  redis:
    # Channel layer backend
    image: redis:latest

  report_event_collector:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py event_collector
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./src/os2datascanner/projects/report/media:/code/uploads/report
    depends_on: *report_dependencies

  report_result_collector:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py result_collector
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *report_dependencies

  report_email_tagger:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py email_tagger
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *report_dependencies

  report_cron:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: supercronic /code/docker/crontab
    environment:
      - TZ=Europe/Copenhagen
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      report_migrate:
        condition: service_completed_successfully
      db:
        condition: service_healthy
      queue:
        condition: service_healthy
    profiles:
      - cron

  explorer:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    env_file:
      - ./dev-environment/engine/dev-settings.env
    environment:
      - DELTA_SCAN_QUEUE=conversions_delta
      - FULL_SCAN_QUEUE=conversions_full
    command: explorer
    init: true
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: &engine_dependencies
      queue:
        condition: service_healthy

  worker:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: worker
    init: true
    env_file:
      - ./dev-environment/engine/dev-settings.env
    environment:
      - SCHEDULE_ON_CPU=0
      - ENABLE_RUSAGE=true
      - OSDS_DEBUG_OAUTH2=true
      - QUEUE_PRIORITY=conversions_delta conversions_full os2ds_conversions
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *engine_dependencies


  worker_2:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: worker
    init: true
    env_file:
      - ./dev-environment/engine/dev-settings.env
    environment:
      - SCHEDULE_ON_CPU=0
      - ENABLE_RUSAGE=true
      - OSDS_DEBUG_OAUTH2=true
      - QUEUE_PRIORITY=conversions_full conversions_delta
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *engine_dependencies

  exporter:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: exporter
    env_file:
      - ./dev-environment/engine/dev-settings.env
    init: true
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *engine_dependencies

  docs:
    image: nicksantamaria/mkdocs
    volumes:
      - ./:/data
    command: ["serve", "-a", "0.0.0.0:8000"]
    ports:
      - "9000:8000"

  api_server:
    image: magentaaps/os2datascanner-api:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: api
    command: [
        "gunicorn",
        "--config", "/code/docker/gunicorn-settings.py",
        "--workers", "2", # only two workers in local dev - to save some resources
        "--reload", # restart workers when code changes
        "os2datascanner.server.wsgi"
    ]
    environment:
      -  OS2DS_ADMIN_USER_CONFIG_PATH="/dev-environment/api/dev-settings.toml"
    tty:
      true
    volumes:
      - ./dev-environment/api/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    profiles:
      - api
    networks:
      - traefik

  swagger_ui:
    image: swaggerapi/swagger-ui:v3.41.1
    environment:
      - SWAGGER_JSON_URL=http://localhost:8070/openapi.yaml
    ports:
      - "8075:8080"
    depends_on:
      - api_server
    profiles:
      - api

  prometheus:
    image: prom/prometheus
    volumes:
      - "./dev-environment/prometheus.yml:/etc/prometheus/prometheus.yml:ro"
    ports:
      - "8050:9090"
    links:
      - pushgateway:pushgateway
    profiles:
      - metric

  pushgateway:
    image: prom/pushgateway
    ports:
      - "9091:9091"

  # default user is admin/admin
  grafana:
    image: grafana/grafana
    volumes:
      - "./dev-environment/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro"
      - "./dev-environment/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro"
    ports:
      - "8060:3000"
    profiles:
      - metric

  idp:
    image: magentaaps/simplesamlphp:master
    environment:
    # TODO: Remove simple saml
#      - SIMPLESAMLPHP_BASEURLPATH=http://localhost:8080/simplesaml/
#      - SIMPLESAMLPHP_SP_ENTITY_ID=http://localhost:8040/saml2_auth/acs/
#      - SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost:8040/saml2_auth/acs/

      # Be aware these require an osdatascanner realm etc.
      - SIMPLESAMLPHP_SP_ENTITY_ID=http://localhost:8090/auth/realms/osdatascanner
      - SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost:8090/auth/realms/osdatascanner/broker/SAML-SSO/endpoint
      - SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost:8090/auth/realms/osdatascanner/broker/SAML-SSO/endpoint
    volumes:
      - ./dev-environment/authsources.php:/var/www/simplesamlphp/config/authsources.php
    ports:
      - "8080:8080"
    extra_hosts:
      # Adds an entry to container's /etc/hosts, allowing it to use localhost-urls.
      # Needed to be able to communicate with keycloak through 'localhost' URL (it's annoying to
      # change SP urls in the environment variables)
      - "localhost:host-gateway"
    profiles:
      - sso

  postgres-keycloak:
    image: postgres
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: keycloak
    volumes:
      - type: volume
        source: knr-postgres-keycloak-volume
        target: /var/lib/postgresql/data
    profiles:
      - ldap
      - sso

  localkeycloak.os2datascanner:
    image: quay.io/keycloak/keycloak:26.0.5

    # Import of realm is done with migration commands below
    # because we need to predefine the Master realm.
    # Using the KEYCLOAK_IMPORT env variable or the -Dkeycloak.import flag will not
    # work, as it will create a Master realm, then try but fail to create a
    # new Master realm from our json file.
    # The Quarkus-based Keycloak version (from 17.0.0) automatically
    # imports any realm files located in /opt/keycloak/data/import/ during startup - but it's
    # the same undesirable behaviour as mentioned above. Hence, we're sticking to migration commands

    command: [

      "start-dev", # Starts in dev mode - should not be used in production.
      "--http-port", "8090", # Quarkus distribution uses 8080 as the default port, but we like 90

      # Quarkus distribution removed /auth from URl's
      # ... but we're not quite ready to remove it ourselves yet, due to differences in dev/prod
      # When production Keycloak is ready to be upgraded too, we can remove this and adjust
      # API calls accordingly.
      "--http-relative-path", "/auth",

      #  Below options are kinda legacy, you could use:   "--import-realm",
      # ... and mount to keycloaks default import path, but then you're not able to
      # override the master realm, which we do. There's quite a debate online regarding this.
      "-Dkeycloak.migration.action=import",
      "-Dkeycloak.migration.provider=singleFile",
      "-Dkeycloak.migration.file=/realm.json",
      # migration.strategy=OVERWRITE_EXISTING will drop & create
      # migration.strategy=IGNORE_EXISTING will not import if a realm of this name already exists
      "-Dkeycloak.migration.strategy=IGNORE_EXISTING",
      ]
    ports:
      - 8090:8090
    extra_hosts:
      # Adds an entry to container's /etc/hosts, allowing it to use localhost-urls.
      # Useful for fetching metadata from our idp service, without having to type idp:8080 url.
      - "localhost:host-gateway"
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      # PROXY_ADDRESS_FORWARDING: "true"
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: keycloak
      KC_DB_URL: jdbc:postgresql://postgres-keycloak/keycloak
      KC_DB: postgres
      KC_DB_SCHEMA: public
      KC_LOG_LEVEL: DEBUG
    volumes:
      - ./dev-environment/realm.json:/realm.json
    depends_on:
      - postgres-keycloak
    profiles:
      - ldap
      - sso

  ldap-server:
    image: osixia/openldap
    environment:
      LDAP_ADMIN_PASSWORD: testMAG
      LDAP_BASE_DN: dc=magenta,dc=test
      LDAP_ORGANISATION: Magenta
      LDAP_DOMAIN: magenta.test
    ports:
      - 389:389
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
    profiles:
      - ldap

  ldap-server-admin:
    image: osixia/phpldapadmin
    ports:
    - 8100:80
    environment:
      PHPLDAPADMIN_LDAP_HOSTS: ldap-server
      PHPLDAPADMIN_HTTPS: 'false'
    depends_on:
      - ldap-server
    profiles:
      - ldap

  traefik:
    image: traefik:v3.0
    ports:
      - "8070:80"
    volumes:
      - ./dev-environment/traefik/:/etc/traefik
    networks:
      - traefik

  samba:
    image: magentaaps/samba-test:master
    volumes:
      - ./dev-environment/data:/mnt
    ports:
      - 8139:139
      - 8445:445
    environment:
      SMB_USER: os2
      SMB_PASSWD: swordfish
      SMB_SHARE_NAME: e2test
      SMB_SHARE_PATH: /mnt
      SMB_SHARE_BROWSABLE: "no"
      SMB_SHARE_READONLY: "yes"

  nginx:
    image: nginx
    volumes:
      - ./dev-environment/data:/usr/share/nginx/html
      - ./dev-environment/nginx_default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - 8910:80

  datasynth:
    image: egnmagenta/os2datasynth:v0.2.5
    environment:
      - DATASYNTH_HOST=datasynth
    ports:
      - 5010:5010

  mailhog:
    image: mailhog/mailhog:v1.0.0
    ports:
      - 8025:8025

x-disabled:
  processor:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: processor --enable-metrics
    init: true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue

  matcher:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: matcher --enable-metrics
    init: true
    restart: unless-stopped
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue

  tagger:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: tagger --enable-metrics
    init: true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue


volumes:
  pgdata_16:
  frontend-bundles:
  postgres-data:
  postgres-initdb.d:
  knr-postgres-keycloak-volume:
  ldap_data:
  ldap_config:
